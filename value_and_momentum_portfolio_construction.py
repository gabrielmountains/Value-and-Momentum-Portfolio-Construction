# -*- coding: utf-8 -*-
"""Value and Momentum Portfolio Construction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-1Bir-4cjQrK7A7m2ncX6IAuJi7Y_9rg

# Value and Momentum Portfolio Construction

This code will create portfolios based on E/P and create portfolios based on momentum. We will then walk through various ways to combine these factors into a single portfolio.

As usual, we import Pandas and Numpy.  I'm also going to use a nice function called MonthEnd, which I will explain below.
"""

import pandas as pd
import numpy as np
from pandas.tseries.offsets import MonthEnd

"""Here I'm reading in CRSP and Compustat data.  We have used these before. Make sure you have installed pyarrow to read the feather format."""

stocks = pd.read_feather('crsp_monthly_stocks.feather')
cstat  = pd.read_feather('compustat_annual.feather')

"""Let's take a look at the stocks dataframe first.  This is data from CRSP, which contains stock returns (RET), closing prices (PRC), volume (VOL), shares outstanding (SHROUT), a code describing the issue type (SHRCD), a code for the primary exchange (EXCHCD), and an industry code (SICCD).

Firms are identified by PERMNO, which remains constant over a firm's life.  Data are monthly, and the date is equal to the last trading day of the month.
"""

stocks.head(10)

"""We're going to clean up the data a bit.  The code below does the following:

1. Shift the date so that it is always the last day of the month, rather than the last trading day.  This will make it easier to merge in with other datasets.
2. Take the absolute value of the closing price.  For shares that don't trade, CRSP sets the price equal to the closing bid-ask midpoint, but it makes the price negative as a warning about this.
3. Define market value (MV) as the product of shares outstanding and closing price.
4. Drop shares outstanding, which we won't use again, and the share code.  We use the share code when we download data from WRDS.  Selecting share codes of 10 or 11 means that we will be downloading common equity and not other securities (ETFs, REITS, etc.).  Also drop EXCHCD, SICCD, PRC, and VOL to make the dataframe easier to display.
5. Set the index to PERMNO/DATE.
6. Sort by the index.
7. Look at the dataframe.
"""

stocks['DATE'] = stocks['DATE'] + MonthEnd(0)
stocks['PRC']  = np.abs(stocks['PRC'])
stocks['MV'] = stocks['SHROUT']*stocks['PRC']
stocks.drop(['SHROUT','SHRCD','EXCHCD','SICCD','PRC','VOL'], axis=1, inplace=True)
stocks.set_index(['PERMNO','DATE'], inplace=True)
stocks.sort_index(inplace=True)
stocks.head()

"""Looks good.  Now let's look at the Compustat data.  This is annual.  It contains a variety of variables that are described in compustat_variables.xlsx.  The one we will use here is earnings before extraordinary items (IB).  For one small purpose I will also look at book equity (SEQ).

Since I used the CRSP/Compustat merged version of Compustat data, I have a variable LPERMNO that is equivalent to the PERMNO variable in CRSP.  Dates in this file represent the last date of the fiscal year.  They are _not_ the dates at which the data became public.
"""

cstat.head(5)

"""Let's rename LPERMNO to PERMNO:"""

cstat.rename(columns={"LPERMNO":"PERMNO"}, inplace=True)
cstat.head(5)

"""To use this data, we must make an assumption about the first date on which this data would be available.  Standard practice is to assume that by 6 months after the fiscal year end we will for sure have access to the annual report.  

I therefore create a new column, DATE, that is designed to represent the date that the data become known.  In the first line I set date equal to the fiscal year end plus six months.  In the second line I make sure that the date is the last day of the month.  As before, this will help when I merge this with the other datasets.
"""

cstat['DATE'] = cstat['DATADATE'] + MonthEnd(0)
cstat.head()

"""With the date defined how I want it, we are ready to set indexes and sort:"""

cstat.set_index(['PERMNO','DATE'], inplace=True)
cstat.sort_index(inplace=True)
cstat.head()

"""We now need to merge these data.  Unfortunately, the data occasionally have multiple rows with the same PERMNO and DATE.  So we are going to have to eliminate duplicate PERMNO/DATE pairs.

There are many ways to do this.  My thought here is that we should assume that if there is more than one PERMNO on the same date, then the bigger one is probably more important and therefore more likely to be correct.

I am therefore going to sort the dataframe in ascending order by PERMNO, then in ascending order by DATE, and then in descending order by size (either MV or SEQ).  This is how you do it:
"""

stocks = stocks.sort_values(by = ['PERMNO','DATE','MV'], ascending = [True, True, False])
cstat  = cstat.sort_values(by = ['PERMNO','DATE','SEQ'], ascending = [True, True, False])

"""Since the first observation for each PERMNO/DATE is the one I want to keep, I eliminate duplicates as follows:"""

stocks = stocks.groupby(['PERMNO','DATE']).head(1)
cstat  = cstat.groupby(['PERMNO','DATE']).head(1)

"""Let's create momentum here.  I will first lag returns two months, and then I will compute the 11-month moving average of the lagged returns. This will create the version of momentum that skips a month between the formation period and the holding period.

Note the use of droplevel in the second step.  
"""

stocks['lag2 RET'] = stocks['RET'].groupby('PERMNO').shift(2)
stocks['momentum'] = stocks['lag2 RET'].groupby('PERMNO').rolling(11).mean().droplevel(0)

"""In the previous step, you will notice I used the droplevel method.  This is because of some unexpected behavior by groupby.  If I don't use droplevel, I get the following result:"""

stocks['lag2 RET'].groupby('PERMNO').rolling(11).mean()

"""For some reason, groupby creates a redundant PERMNO index.  This is eliminated by droplevel(0), which drops the first (which is 0) level of the index.

Now it's time to combine the CRSP and Compustat data.  Because of the way we have indexed each dataframe, specifically making sure that all dates are the last days of the month, this is super easy.  Just type:

stocks[['IB','SEQ']]   = cstat[['IB','SEQ']]

However, this is a bit slow.  A much faster alternative is to use the merge method:
"""

stocks = stocks.merge(cstat[['IB','SEQ']], how='left', on=['PERMNO','DATE'])

"""To see what happened, let's take a look at one stock, Apple, which has PERMNO=14593:"""

stocks.loc[14593].tail(24)

"""Apple's fiscal year ends in September.  That's why we see IB in those months and no others.

Now we will compute the E/P ratio in two different ways.

The first is to use the most recent known value of E (column IB) and divide it by the contemporaneous observation of P (column MV), meaning the value of MV that corresponds to the most recent fiscal year end.  We will then lag the ratio 6 months to account for the fact that earnings are not known for some time after the end of the quarter.

We will need to multiply the E/P ratio by 1000 for it to make sense.  The reason is that CRSP and Compustat are in different units.  In CRSP, the shares outstanding series used to create market values (MV) was in 1000s of shares.  Thus, the MV column is too small by a factor of 1000.  In Compustat, earnings (IB) are in millions of dollars.  Multiplying by 1000 makes these numbers comparable.
"""

stocks['lag EP v1'] = stocks['IB'].groupby('PERMNO').shift(6) / stocks['MV'].groupby('PERMNO').shift(6) * 1000

"""Looking again at Apple, we can see what we have done:"""

stocks.loc[14593].tail(24)

"""The calculations look right, but they only result in one E/P ratio per year.  We will fill in the rest using the _fillna_ method with the _pad_ option.  This uses older data to fill in for missing values.  Because older data are used, we don't have to worry about look-ahead bias.  The groupby('PERMNO') step makes sure that we never fill in one firm's earnings with those of another firm.  The limit=15 option says that we will not use a prior value if it is more than 15 months old, which should be unusual situations."""

stocks['lag EP v1']  = stocks['lag EP v1'].groupby('PERMNO').fillna(method='pad', limit=15)

"""The second approach will be to use the most recent known E divided by the most recent known P, even if these two variables are observed at very different times.

To start with this, lets compute the most recent E (column IB) that we would observe.  We'll then use fillna in the same way to fill in missing values with older data.
"""

stocks['lag IB']  = stocks['IB'].groupby('PERMNO').shift(6).fillna(method='pad', limit=15)

"""Again taking a look at Apple, it seems to have worked as expected:"""

stocks.loc[14593].tail(24)

"""To compute the E/P ratio in this approach, we divide the lagged IB column by the most recently observed value of MV, which is the value in the previous row:"""

stocks['lag EP v2'] = stocks['lag IB'] / stocks['MV'].groupby('PERMNO').shift(1) * 1000

"""Again, Apple:"""

stocks.loc[14593].tail(24)

"""Note that the two E/P ratios are not the same.

Our primary analysis will be on the RET, lag EP v1, lag EP v2, and momentum columns.  Let's make sure all four variables are observed:
"""

stocks = stocks.dropna(subset=['RET','lag EP v1','lag EP v2','momentum'])

"""Since we are going to combine different firms, on the same date, into portfolios, the next step is to sort by DATE first and then by PERMNO."""

stocks = stocks.reorder_levels(['DATE','PERMNO'])
stocks.sort_index(inplace=True)

"""One final step before portfolios are constructed is to eliminate observations that look funny.  These could be the result of earnings being close to zero.  They could also be database errors or errors in merging the different databases.  

In any case, it is entirely feasible for an investor to say that he or she is not going to hold stocks that have P/E ratios below 5 or above 100, so I will exclude all stocks that are outside that range (using either P/E measure).  

I also remove stocks with momentum below -.1 or above .1.  These are very extreme values.  Removing them has little effect on the results below, though it does improve them slightly.
"""

stocks = stocks.loc[(stocks['lag EP v1']>0)  & (stocks['lag EP v2']>0)  & \
                    (stocks['lag EP v1']<.5) & (stocks['lag EP v2']<.5) & \
                    (stocks['momentum']>-.1) & (stocks['momentum']<.1)]

"""Finally, time to compute portfolios.  I will compute quintile portfolios using the rank method. This will split the sample into percentile groups based upon the input variable. I can then transform the percentiles into an integer group number.

However, I do NOT want to split the sample into groups across all dates. Rather, I need to, for each date, split the sample into five groups by the P/E measures. I can do this using the "groupby" command.

We're going to look at quintiles formed on two different measures.  Rather than type all the code twice, let's create functions that do all the necessary calculations.  The first one examines quintile portfolios and returns the HML long/short portfolio.  The second computes performance statistics for the HML portfolio returns.
"""

# the function takes the name of the sorting variable we want to use as its single input
def perf(sortvar):

    # creating the quintiles
    stocks['Q'] = (stocks[sortvar].groupby('DATE').rank(pct=True)*5 - 0.00001).astype(int) +1

    # computing quintile portfolio returns
    ports = stocks.groupby(['Q','DATE'])['RET'].mean()

    # printing out basic statistics on each portfolio
    print(ports.groupby('Q').describe())

    # computing high minus low portfolios
    hml = ports.loc[5] - ports.loc[1]

    return hml


def stats(hml):
    # printing basic statistics plus sharpe and t-stat
    stats = hml.describe()
    stats.loc['tstat']  = stats.loc['mean'] / stats.loc['std'] * np.sqrt(stats.loc['count'])
    stats.loc['sharpe'] = stats.loc['mean'] / stats.loc['std'] * np.sqrt(12)
    print(stats)

"""Now we just have to call the function twice:"""

hml_ep = perf('lag EP v2')
stats(hml_ep)

hml_mom = perf('momentum')
stats(hml_mom)

"""__Combining Portfolios__

One way to combine the value and momentum strategies is just to put half of your money in each long/short portfolio.  The performance of this strategy is as follows:
"""

stats(.5*hml_ep + .5*hml_mom)

"""An alternative way to combine signals is to normalize them and take the sum, i.e.
$$ score_{i,t} = \frac{ep_{i,t} - \mu^{ep}_t}{\sigma^{ep}_t} + \frac{mom_{i,t} - \mu^{mom}_t}{\sigma^{mom}_t} ,$$
where $\mu^x_t$ and $\sigma^x_t$ are the mean and SD of $x$ across all firms at date $t$.

Note that subtracting the means affects the score, but it does not change the rankings of different stocks.  So in the next calculation, I don't bother to remove them.
"""

stocks['score'] = stocks['lag EP v2']/stocks['lag EP v2'].groupby('DATE').std() + stocks['momentum']/stocks['momentum'].groupby('DATE').std()

"""Now I form quintile portfolios based on $score$:"""

hml_score = perf('score')
stats(hml_score)

"""Another possibility is to do a two-way sort.  I will do independent quintile sorts on EP and momentum.  I will then go long stocks that are in the 5th quintile of both variables and short stocks that are in both 1st quintiles."""

# creating the quintiles
stocks['Qep']  = (stocks['lag EP v2'].groupby('DATE').rank(pct=True)*5 - 0.00001).astype(int) +1
stocks['Qmom'] = (stocks['momentum'].groupby('DATE').rank(pct=True)*5 - 0.00001).astype(int) +1

# computing quintile portfolio returns
ports = stocks.groupby(['Qep','Qmom','DATE'])['RET'].mean()

# printing out basic statistics on each portfolio
print(ports.groupby(['Qep','Qmom']).describe())

# computing high minus low portfolios
hml = ports.loc[5,5] - ports.loc[1,1]

stats(hml)

